{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk,download\n",
    "import os\n",
    "import collections\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pickle\n",
    "from collections import Iterable\n",
    "from nltk.tag import  ClassifierBasedTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions helps us parse the file and create the chunker.\n",
    "<br>\n",
    "-__read_gmb__ :reads the directory that the files exists. Checks and includes only the words with the required tags and dismisses the other words.\n",
    "<br>\n",
    "-__transform__ :Insert I and B on tags (inner of a segment and beginning of segment).\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ner_tags=collections.Counter()\n",
    "\n",
    "def read_gmb(corpus_root):\n",
    "    print 'About to read from file in ',corpus_root\n",
    "    #Move in every folder of the directory.\n",
    "    for root, dirs, files in os.walk(corpus_root):\n",
    "        for filename in files:\n",
    "            #Keep only the .tags files.\n",
    "            if filename.endswith(\".tags\"):\n",
    "                with open(os.path.join(root, filename), 'rb') as file_handle:\n",
    "                    file_content = file_handle.read().decode('utf-8').strip()\n",
    "                    #Sentences are separated by two newlines.\n",
    "                    annotated_sentences = file_content.split('\\n\\n')\n",
    "                    for annotated_sentence in annotated_sentences:\n",
    "                        #Words are separated by a newline.\n",
    "                        annotated_tokens = [seq for seq in annotated_sentence.split('\\n') if seq]\n",
    "                        standard_form_tokens = []\n",
    "\n",
    "                        for idx, annotated_token in enumerate(annotated_tokens):\n",
    "                            #Each word is  separated via tab by its annotation\n",
    "                            annotations = annotated_token.split('\\t')\n",
    "                            word, tag, ner = annotations[0], annotations[1], annotations[3]\n",
    "                            #As the exercises tells,keep only the most important-primary categories.\n",
    "                            #Case where there is an interesting notation ,split the subcategories(geo-nam->geo) and kee only the \n",
    "                            #bigger categories.\n",
    "                            if ner != 'O':\n",
    "                                ner = ner.split('-')[0]\n",
    "                                ner_tags[ner] += 1\n",
    "                            standard_form_tokens.append((word, tag, ner))\n",
    "                        b_i_ = transform(standard_form_tokens)\n",
    "                        # The naive bayes classifier version must get as input ((word,tag),iob_notation)\n",
    "                        yield [((w, t), iob) for w, t, iob in b_i_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform(annotated_sentence):\n",
    "    #[(w1, t1, iob1), ...] with not proper format that would be transformed to proper(B- will be added on the beginning of the IOB notation \n",
    "    #I_ will be added on the inner segment elements.)\n",
    "    not_ = []\n",
    "    #For each word we meet in a sentence\n",
    "    for idx, annotated_token in enumerate(annotated_sentence):\n",
    "        t, wrd, category = annotated_token\n",
    "        if category != 'O': #O are the objects annotated as outer to any chunk.\n",
    "            #if previous item notation of the segment  is B-,then the following item should be inner and add I- to the beginning\n",
    "            if annotated_sentence[idx - 1][2] == category:\n",
    "                category = \"I-\" + category\n",
    "            else:         #Otherwise it is the B- case where the item is in the beginning of a segment \n",
    "                category = \"B-\" + category\n",
    "        not_.append((t, wrd, category))\n",
    "    return not_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class NamedEntityChunker is child class of ChunkParse. It has two fields: \n",
    "<br>\n",
    "__feature_detector__ : used for identifying the features.It takes a word and gets valuable information such as the previous,next words etc.\n",
    "<br>\n",
    "__tagger__ : Naive-Bayes Classifier used to predict the different sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NamedEntityChunker(ChunkParserI):\n",
    "    def __init__(self, train_sents, **kwargs):\n",
    "        #The function features that will be used for each instance to return its features.\n",
    "        self.feature_detector = features\n",
    "        self.tagger = ClassifierBasedTagger(train=train_sents,feature_detector=features,**kwargs)\n",
    " \n",
    "    def parse(self, tagged_sent):\n",
    "        #Call the classifier for the input and keep the result in chunks variable.\n",
    "        chunks = self.tagger.tag(tagged_sent)\n",
    "        #Perform the opposite than the previous procedure: ((word,tag),notation)->(word,tag,notation)\n",
    "        iob_triplets = [(w, t, c) for ((w, t), c) in chunks]\n",
    "        return conlltags2tree(iob_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repr_(word):\n",
    "    str_=''\n",
    "    for x in word:\n",
    "        if x.isupper()==True:\n",
    "            str_=str_+'X'\n",
    "        else:\n",
    "            str_=str_+'x'\n",
    "        return str_\n",
    "\n",
    "def features(tokens, index, history):\n",
    "    #Features contain different properties tha may be useful to be known\n",
    "    # init the stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    # Pad the sequence with placeholders\n",
    "    tokens = [('[START2]', '[START2]'), ('[START1]', '[START1]')] + list(tokens) + [('[END1]', '[END1]'), ('[END2]', '[END2]')]\n",
    "    history = ['[START2]', '[START1]'] + list(history)\n",
    "    index += 2 \n",
    "    word, pos = tokens[index] #Actual word\n",
    "    prevword, prevpos = tokens[index - 1]  #Previous word/Previous index\n",
    "    prevprevword, prevprevpos = tokens[index - 2] #Prev-previous word,prev-previous index\n",
    "    nextword, nextpos = tokens[index + 1] #Next word,next index\n",
    "    nextnextword, nextnextpos = tokens[index + 2] #Next-next word,next-next index\n",
    "    prefix1=word[0] #The first letter of the word\n",
    "    suffix1=word[-1] #The last letter of the world\n",
    "    previob = history[index - 1]  #The previous IOB tag assigned.\n",
    "    allascii = all([True for c in word if c in string.ascii_lowercase]) #All lowercase\n",
    "    repr_w=repr_(word) #representation of current,previous and next words. Representation is of form X for capital\n",
    "                        #x for lowercase.\n",
    "    prev_repr=repr_(prevword)\n",
    "    next_repr=repr_(nextword)\n",
    "   \n",
    "    return {\n",
    "        'word': word,\n",
    "        'lemma': stemmer.stem(word),\n",
    "        'pos': pos,\n",
    "        'all-ascii': allascii,\n",
    "        'next-word': nextword,\n",
    "        'next-lemma': stemmer.stem(nextword),\n",
    "        'next-pos': nextpos,\n",
    "        'next-next-word': nextnextword,\n",
    "        'nextnextpos': nextnextpos,\n",
    "        'prev-word': prevword,\n",
    "        'prev-lemma': stemmer.stem(prevword),\n",
    "        'prev-pos': prevpos,\n",
    "        'prev-prev-word': prevprevword,\n",
    "        'prev-prev-pos': prevprevpos,\n",
    "        'prev-iob': previob,\n",
    "        'prefix':prefix1,\n",
    "        'suffix':suffix1,\n",
    "        'representation':repr_w,\n",
    "        'prev-repr':prev_repr,\n",
    "        'next-repr':next_repr\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to read from file in  /media/sf_mine/gmb-2.2.0/\n",
      "Counter({u'geo': 116776, u'org': 96188, u'per': 88508, u'tim': 69578, u'gpe': 41360, u'art': 1734, u'eve': 1418, u'nat': 600})\n"
     ]
    }
   ],
   "source": [
    "root='/media/sf_mine/gmb-2.2.0/'\n",
    "reader = read_gmb(root)\n",
    "data = list(reader)\n",
    "print ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train instances = 49608\n",
      "Test instances  = 12402\n"
     ]
    }
   ],
   "source": [
    "tr= data[:int(len(data) * 0.8)] #80 percent of the dataset will be the training sample\n",
    "te = data[int(len(data) * 0.8):]#20 percent of the dataset will be from the test sample\n",
    " \n",
    "print \"Train instances = %s\" % len(tr) \n",
    "print \"Test instances  = %s\" % len(te) \n",
    "chunker = NamedEntityChunker(tr[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.925805578121\n"
     ]
    }
   ],
   "source": [
    "score = chunker.evaluate([conlltags2tree([(w, t, iob) for (w, t), iob in iobs]) for iobs in te[:500]])\n",
    "print score.accuracy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
